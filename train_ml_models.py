#!/usr/bin/env python3
"""
ğŸ¤– ENTRAÃNEMENT DES MODÃˆLES ML
================================================================================
EntraÃ®ne les modÃ¨les de prÃ©diction de qualitÃ© de l'air
================================================================================
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import pickle
import json
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Imports ML
from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer

# ModÃ¨les avancÃ©s
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost non disponible - installation: pip install xgboost")

try:
    from sklearn.neural_network import MLPRegressor
    NEURAL_AVAILABLE = True
except ImportError:
    NEURAL_AVAILABLE = False

from app.core.logging import get_logger

logger = get_logger(__name__)

class AirQualityMLTrainer:
    """EntraÃ®nement des modÃ¨les de prÃ©diction air quality"""
    
    def __init__(self, output_dir: str = "models/trained"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.models = {}
        self.scalers = {}
        self.imputers = {}
        self.metrics = {}
        
        # Configuration des modÃ¨les
        self.model_configs = {
            'random_forest': {
                'class': RandomForestRegressor,
                'params': {
                    'n_estimators': 100,
                    'max_depth': 10,
                    'random_state': 42,
                    'n_jobs': -1
                }
            },
            'gradient_boosting': {
                'class': GradientBoostingRegressor,
                'params': {
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1,
                    'random_state': 42
                }
            }
        }
        
        # Ajout XGBoost si disponible
        if XGBOOST_AVAILABLE:
            self.model_configs['xgboost'] = {
                'class': xgb.XGBRegressor,
                'params': {
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1,
                    'random_state': 42
                }
            }
        
        # Ajout Neural Network si disponible
        if NEURAL_AVAILABLE:
            self.model_configs['neural_network'] = {
                'class': MLPRegressor,
                'params': {
                    'hidden_layer_sizes': (100, 50),
                    'max_iter': 500,
                    'random_state': 42
                }
            }
    
    def load_data(self, features_file: str, targets_file: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Charge les donnÃ©es processÃ©es"""
        logger.info(f"ğŸ“‚ Chargement donnÃ©es: {features_file}, {targets_file}")
        
        features_df = pd.read_csv(features_file)
        targets_df = pd.read_csv(targets_file)
        
        # Conversion timestamp si prÃ©sent
        if 'timestamp' in features_df.columns:
            features_df['timestamp'] = pd.to_datetime(features_df['timestamp'])
        
        logger.info(f"âœ… Features: {features_df.shape}, Targets: {targets_df.shape}")
        return features_df, targets_df
    
    def preprocess_data(self, features_df: pd.DataFrame, targets_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
        """PrÃ©processing des donnÃ©es"""
        logger.info("ğŸ”§ PrÃ©processing des donnÃ©es")
        
        # Nettoyage des donnÃ©es initiales
        logger.info(f"ğŸ“Š Shape initiale: features{features_df.shape}, targets{targets_df.shape}")
        
        # Pour les features: exclure timestamp et colonnes avec que des NaN
        feature_columns = []
        for col in features_df.columns:
            if col != 'timestamp' and features_df[col].dtype in ['int64', 'float64']:
                # Garder seulement les colonnes qui ont des valeurs non-NaN
                if not features_df[col].isna().all():
                    feature_columns.append(col)
        
        X = features_df[feature_columns].copy()
        
        # Pour les targets: exclure colonnes avec que des NaN
        target_columns = []
        for col in targets_df.columns:
            if not targets_df[col].isna().all():
                target_columns.append(col)
        
        y = targets_df[target_columns].copy()
        
        logger.info(f"ğŸ“Š Colonnes conservÃ©es: features={len(feature_columns)}, targets={len(target_columns)}")
        logger.info(f"ğŸ“Š Features: {feature_columns[:10]}...")  # Premiers 10
        logger.info(f"ğŸ“Š Targets: {target_columns}")
        
        # Gestion des valeurs manquantes - Features
        self.imputers['features'] = SimpleImputer(strategy='median')
        X_imputed = self.imputers['features'].fit_transform(X)
        X = pd.DataFrame(X_imputed, columns=feature_columns)
        
        # Gestion des valeurs manquantes - Targets
        self.imputers['targets'] = SimpleImputer(strategy='median')
        y_imputed = self.imputers['targets'].fit_transform(y)
        y = pd.DataFrame(y_imputed, columns=target_columns)
        
        # Normalisation des features
        self.scalers['features'] = StandardScaler()
        X_scaled = self.scalers['features'].fit_transform(X)
        
        logger.info(f"âœ… PrÃ©processing terminÃ©: X{X_scaled.shape}, y{y.shape}")
        return X_scaled, y.values, feature_columns, target_columns
    
    def train_models(self, X: np.ndarray, y: np.ndarray, 
                     feature_columns: List[str], target_columns: List[str]) -> Dict:
        """EntraÃ®ne tous les modÃ¨les"""
        logger.info("ğŸš€ DÃ©but entraÃ®nement des modÃ¨les")
        
        results = {}
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # EntraÃ®nement pour chaque target (polluant)
        for target_idx, target_name in enumerate(target_columns):
            logger.info(f"ğŸ¯ EntraÃ®nement pour: {target_name}")
            
            y_train_target = y_train[:, target_idx]
            y_test_target = y_test[:, target_idx]
            
            target_results = {}
            
            # EntraÃ®nement de chaque modÃ¨le
            for model_name, config in self.model_configs.items():
                try:
                    logger.info(f"   ğŸ”„ ModÃ¨le: {model_name}")
                    
                    # CrÃ©ation et entraÃ®nement du modÃ¨le
                    model = config['class'](**config['params'])
                    model.fit(X_train, y_train_target)
                    
                    # PrÃ©dictions
                    y_pred_train = model.predict(X_train)
                    y_pred_test = model.predict(X_test)
                    
                    # MÃ©triques
                    metrics = {
                        'train_rmse': np.sqrt(mean_squared_error(y_train_target, y_pred_train)),
                        'test_rmse': np.sqrt(mean_squared_error(y_test_target, y_pred_test)),
                        'train_mae': mean_absolute_error(y_train_target, y_pred_train),
                        'test_mae': mean_absolute_error(y_test_target, y_pred_test),
                        'train_r2': r2_score(y_train_target, y_pred_train),
                        'test_r2': r2_score(y_test_target, y_pred_test)
                    }
                    
                    # Sauvegarde du modÃ¨le
                    model_file = self.output_dir / f"{target_name}_{model_name}_model.pkl"
                    with open(model_file, 'wb') as f:
                        pickle.dump(model, f)
                    
                    target_results[model_name] = {
                        'model_file': str(model_file),
                        'metrics': metrics
                    }
                    
                    logger.info(f"   âœ… {model_name}: RÂ²={metrics['test_r2']:.3f}, RMSE={metrics['test_rmse']:.3f}")
                    
                except Exception as e:
                    logger.error(f"   âŒ Erreur {model_name}: {str(e)}")
                    target_results[model_name] = {'error': str(e)}
            
            results[target_name] = target_results
        
        # Sauvegarde des preprocessors
        preprocessors_file = self.output_dir / "preprocessors.pkl"
        with open(preprocessors_file, 'wb') as f:
            pickle.dump({
                'scalers': self.scalers,
                'imputers': self.imputers,
                'feature_columns': feature_columns,
                'target_columns': target_columns
            }, f)
        
        logger.info(f"âœ… Preprocessors sauvegardÃ©s: {preprocessors_file}")
        
        return results
    
    def save_training_report(self, results: Dict, features_df: pd.DataFrame, targets_df: pd.DataFrame):
        """Sauvegarde le rapport d'entraÃ®nement"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = {
            'timestamp': timestamp,
            'data_info': {
                'features_shape': features_df.shape,
                'targets_shape': targets_df.shape,
                'features_columns': list(features_df.columns),
                'targets_columns': list(targets_df.columns)
            },
            'models_trained': list(self.model_configs.keys()),
            'results': results,
            'summary': self._create_summary(results)
        }
        
        report_file = self.output_dir / f"training_report_{timestamp}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"ğŸ“Š Rapport sauvegardÃ©: {report_file}")
        return report_file
    
    def _create_summary(self, results: Dict) -> Dict:
        """CrÃ©e un rÃ©sumÃ© des performances"""
        summary = {
            'best_models': {},
            'average_performance': {},
            'total_models_trained': 0
        }
        
        for target_name, target_results in results.items():
            best_r2 = -float('inf')
            best_model = None
            
            valid_results = []
            
            for model_name, model_result in target_results.items():
                if 'metrics' in model_result:
                    summary['total_models_trained'] += 1
                    metrics = model_result['metrics']
                    
                    if metrics['test_r2'] > best_r2:
                        best_r2 = metrics['test_r2']
                        best_model = model_name
                    
                    valid_results.append(metrics)
            
            if best_model:
                summary['best_models'][target_name] = {
                    'model': best_model,
                    'r2_score': best_r2
                }
            
            if valid_results:
                avg_metrics = {}
                for metric in ['test_r2', 'test_rmse', 'test_mae']:
                    values = [r[metric] for r in valid_results]
                    avg_metrics[metric] = np.mean(values)
                
                summary['average_performance'][target_name] = avg_metrics
        
        return summary

def find_latest_data_files(data_dir: str = "data/processed") -> Tuple[Optional[str], Optional[str]]:
    """Trouve les fichiers de donnÃ©es les plus rÃ©cents"""
    data_path = Path(data_dir)
    
    if not data_path.exists():
        return None, None
    
    features_files = list(data_path.glob("features_*.csv"))
    targets_files = list(data_path.glob("targets_*.csv"))
    
    if not features_files or not targets_files:
        return None, None
    
    # Tri par date de modification
    features_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    targets_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    
    return str(features_files[0]), str(targets_files[0])

def main():
    """Pipeline principal d'entraÃ®nement"""
    print("ğŸ¤– ENTRAÃNEMENT DES MODÃˆLES ML")
    print("=" * 80)
    print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    try:
        # 1. Recherche des donnÃ©es
        print("\nğŸ” Ã‰TAPE 1: Recherche des donnÃ©es")
        features_file, targets_file = find_latest_data_files()
        
        if not features_file or not targets_file:
            print("âŒ Aucun fichier de donnÃ©es trouvÃ©")
            print("ğŸ’¡ ExÃ©cutez d'abord: python data_pipeline_ml.py")
            return
        
        print(f"âœ… Features: {features_file}")
        print(f"âœ… Targets: {targets_file}")
        
        # 2. Initialisation du trainer
        trainer = AirQualityMLTrainer()
        
        # 3. Chargement des donnÃ©es
        print("\nğŸ“‚ Ã‰TAPE 2: Chargement des donnÃ©es")
        features_df, targets_df = trainer.load_data(features_file, targets_file)
        
        # 4. PrÃ©processing
        print("\nğŸ”§ Ã‰TAPE 3: PrÃ©processing")
        X, y, feature_columns, target_columns = trainer.preprocess_data(features_df, targets_df)
        
        # 5. EntraÃ®nement
        print("\nğŸš€ Ã‰TAPE 4: EntraÃ®nement des modÃ¨les")
        results = trainer.train_models(X, y, feature_columns, target_columns)
        
        # 6. Sauvegarde du rapport
        print("\nğŸ“Š Ã‰TAPE 5: GÃ©nÃ©ration du rapport")
        report_file = trainer.save_training_report(results, features_df, targets_df)
        
        # 7. RÃ©sumÃ©
        print("\n" + "=" * 80)
        print("ğŸ‰ ENTRAÃNEMENT TERMINÃ‰!")
        print("=" * 80)
        
        total_models = sum(len([r for r in target_results.values() if 'metrics' in r]) 
                          for target_results in results.values())
        
        print(f"âœ… ModÃ¨les entraÃ®nÃ©s: {total_models}")
        print(f"âœ… Polluants couverts: {len(target_columns)}")
        print(f"âœ… Algorithmes utilisÃ©s: {len(trainer.model_configs)}")
        
        print(f"\nğŸ¯ Meilleurs modÃ¨les par polluant:")
        for target_name, target_results in results.items():
            best_r2 = -1
            best_model = "Aucun"
            
            for model_name, model_result in target_results.items():
                if 'metrics' in model_result:
                    r2 = model_result['metrics']['test_r2']
                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = model_name
            
            if best_r2 > -1:
                print(f"   â€¢ {target_name}: {best_model} (RÂ²={best_r2:.3f})")
        
        print(f"\nğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
        print(f"   â€¢ ModÃ¨les: {trainer.output_dir}/")
        print(f"   â€¢ Rapport: {report_file}")
        
        print(f"\nğŸ”„ Prochaines Ã©tapes:")
        print(f"   â€¢ Tester les modÃ¨les en prÃ©diction")
        print(f"   â€¢ IntÃ©grer dans l'API de prÃ©diction")
        print(f"   â€¢ Optimiser les hyperparamÃ¨tres")
        
    except Exception as e:
        print(f"âŒ Erreur entraÃ®nement: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()