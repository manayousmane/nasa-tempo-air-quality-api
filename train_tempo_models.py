#!/usr/bin/env python3
"""
ğŸ¤– MODÃˆLES ML SPÃ‰CIALISÃ‰S TEMPO
================================================================================
EntraÃ®nement de modÃ¨les pour prÃ©diction air quality AmÃ©rique du Nord
SystÃ¨me de prÃ©diction gÃ©ographique pour toutes coordonnÃ©es TEMPO
================================================================================
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import pickle
import json
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# ML imports
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge

# GÃ©ospatial
from scipy.spatial.distance import cdist
from scipy.interpolate import griddata

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

from app.core.logging import get_logger

logger = get_logger(__name__)

class TempoMLPredictor:
    """SystÃ¨me de prÃ©diction ML spÃ©cialisÃ© pour TEMPO"""
    
    def __init__(self, models_dir: str = "models/tempo"):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.models = {}
        self.scalers = {}
        self.grid_interpolators = {}
        
        # Configuration modÃ¨les optimisÃ©s pour donnÃ©es gÃ©ospatiales
        self.model_configs = {
            'random_forest': {
                'class': RandomForestRegressor,
                'params': {
                    'n_estimators': 200,
                    'max_depth': 15,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2,
                    'random_state': 42,
                    'n_jobs': -1
                }
            },
            'gradient_boosting': {
                'class': GradientBoostingRegressor,
                'params': {
                    'n_estimators': 150,
                    'max_depth': 8,
                    'learning_rate': 0.1,
                    'subsample': 0.8,
                    'random_state': 42
                }
            },
            'knn_spatial': {
                'class': KNeighborsRegressor,
                'params': {
                    'n_neighbors': 10,
                    'weights': 'distance',
                    'metric': 'haversine'  # OptimisÃ© pour coordonnÃ©es gÃ©ographiques
                }
            },
            'ridge_baseline': {
                'class': Ridge,
                'params': {
                    'alpha': 1.0,
                    'random_state': 42
                }
            }
        }
        
        if XGBOOST_AVAILABLE:
            self.model_configs['xgboost'] = {
                'class': xgb.XGBRegressor,
                'params': {
                    'n_estimators': 150,
                    'max_depth': 8,
                    'learning_rate': 0.1,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'random_state': 42,
                    'n_jobs': -1
                }
            }
        
        # Polluants TEMPO
        self.pollutants = ['pm25', 'pm10', 'no2', 'o3', 'co', 'so2']
        
        # Zone de couverture TEMPO
        self.tempo_bounds = {
            'lat_min': 40.0, 'lat_max': 70.0,
            'lon_min': -130.0, 'lon_max': -70.0
        }
    
    def load_tempo_data(self, data_dir: str = "data/tempo_north_america") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Charge les derniÃ¨res donnÃ©es TEMPO"""
        data_path = Path(data_dir)
        
        # Trouve les fichiers les plus rÃ©cents
        feature_files = list(data_path.glob("tempo_features_*.csv"))
        target_files = list(data_path.glob("tempo_targets_*.csv"))
        
        if not feature_files or not target_files:
            raise FileNotFoundError("Aucun fichier TEMPO trouvÃ©. ExÃ©cutez tempo_pipeline_north_america.py")
        
        # Tri par date
        feature_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        target_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        features_df = pd.read_csv(feature_files[0])
        targets_df = pd.read_csv(target_files[0])
        
        logger.info(f"ğŸ“‚ DonnÃ©es chargÃ©es: Features{features_df.shape}, Targets{targets_df.shape}")
        return features_df, targets_df
    
    def prepare_spatial_features(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """PrÃ©pare features gÃ©ospatiales optimisÃ©es"""
        df = features_df.copy()
        
        # Conversion coordonnÃ©es en radians pour calculs gÃ©ospatiales
        df['lat_rad'] = np.radians(df['latitude'])
        df['lon_rad'] = np.radians(df['longitude'])
        
        # Features gÃ©ospatiales avancÃ©es
        df['lat_lon_interaction'] = df['latitude'] * df['longitude']
        df['distance_to_center'] = np.sqrt(
            (df['latitude'] - 55.0)**2 + (df['longitude'] + 100.0)**2
        )
        
        # Patterns gÃ©ographiques Nord-AmÃ©ricains
        # CÃ´te Est (influence ocÃ©anique)
        df['east_coast_proximity'] = np.exp(-(df['longitude'] + 75.0)**2 / 100)
        # CÃ´te Ouest (influence Pacifique)
        df['west_coast_proximity'] = np.exp(-(df['longitude'] + 120.0)**2 / 100)
        # RÃ©gion des Grands Lacs
        df['great_lakes_proximity'] = np.exp(-((df['latitude'] - 45.0)**2 + (df['longitude'] + 85.0)**2) / 50)
        
        return df
    
    def train_spatial_models(self, features_df: pd.DataFrame, targets_df: pd.DataFrame) -> Dict:
        """EntraÃ®ne modÃ¨les spÃ©cialisÃ©s pour prÃ©diction spatiale"""
        logger.info("ğŸš€ EntraÃ®nement modÃ¨les TEMPO spÃ©cialisÃ©s")
        
        # VÃ©rification alignement des donnÃ©es
        logger.info(f"ğŸ“Š Shapes initiales: Features{features_df.shape}, Targets{targets_df.shape}")
        
        # Alignement des indices (prendre le minimum)
        min_samples = min(len(features_df), len(targets_df))
        features_df = features_df.iloc[:min_samples].copy()
        targets_df = targets_df.iloc[:min_samples].copy()
        
        logger.info(f"ğŸ“Š Shapes alignÃ©es: Features{features_df.shape}, Targets{targets_df.shape}")
        
        # PrÃ©paration features spatiales
        X = self.prepare_spatial_features(features_df)
        
        # Exclusion des colonnes non-numÃ©riques
        feature_columns = X.select_dtypes(include=[np.number]).columns.tolist()
        X = X[feature_columns]
        
        results = {}
        
        # EntraÃ®nement pour chaque polluant
        for pollutant in self.pollutants:
            target_col = f'{pollutant}_target'
            if target_col not in targets_df.columns:
                logger.warning(f"âš ï¸ {target_col} non trouvÃ©")
                continue
            
            logger.info(f"ğŸ¯ EntraÃ®nement pour: {pollutant}")
            
            y = targets_df[target_col].values
            
            # VÃ©rification finale des shapes
            if len(X) != len(y):
                logger.error(f"âŒ Shape mismatch: X{len(X)} vs y{len(y)}")
                continue
            
            # Suppression des valeurs manquantes
            mask = ~(np.isnan(X.values).any(axis=1) | np.isnan(y))
            X_clean = X[mask]
            y_clean = y[mask]
            
            logger.info(f"   ğŸ“Š DonnÃ©es nettoyÃ©es: {len(X_clean)} Ã©chantillons")
            
            if len(X_clean) < 100:
                logger.warning(f"âš ï¸ Pas assez de donnÃ©es pour {pollutant}: {len(X_clean)}")
                continue
            
            # Split train/test avec stratification gÃ©ographique
            X_train, X_test, y_train, y_test = train_test_split(
                X_clean, y_clean, test_size=0.2, random_state=42
            )
            
            # Normalisation
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            self.scalers[pollutant] = scaler
            
            pollutant_results = {}
            
            # EntraÃ®nement de chaque modÃ¨le
            for model_name, config in self.model_configs.items():
                try:
                    logger.info(f"   ğŸ”„ {model_name}")
                    
                    # CrÃ©ation du modÃ¨le
                    if model_name == 'knn_spatial':
                        # KNN avec coordonnÃ©es gÃ©ographiques directes
                        coords_train = X_train[['latitude', 'longitude']].values
                        coords_test = X_test[['latitude', 'longitude']].values
                        
                        # Conversion en radians pour distance haversine
                        coords_train_rad = np.radians(coords_train)
                        coords_test_rad = np.radians(coords_test)
                        
                        model = config['class'](**config['params'])
                        model.fit(coords_train_rad, y_train)
                        y_pred = model.predict(coords_test_rad)
                    else:
                        # ModÃ¨les standards avec toutes les features
                        model = config['class'](**config['params'])
                        model.fit(X_train_scaled, y_train)
                        y_pred = model.predict(X_test_scaled)
                    
                    # MÃ©triques
                    metrics = {
                        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                        'mae': mean_absolute_error(y_test, y_pred),
                        'r2': r2_score(y_test, y_pred),
                        'samples_train': len(X_train),
                        'samples_test': len(X_test)
                    }
                    
                    # Sauvegarde du modÃ¨le
                    model_file = self.models_dir / f"{pollutant}_{model_name}_tempo.pkl"
                    with open(model_file, 'wb') as f:
                        pickle.dump({
                            'model': model,
                            'scaler': scaler if model_name != 'knn_spatial' else None,
                            'feature_columns': feature_columns,
                            'pollutant': pollutant,
                            'model_type': model_name
                        }, f)
                    
                    pollutant_results[model_name] = {
                        'metrics': metrics,
                        'model_file': str(model_file)
                    }
                    
                    logger.info(f"   âœ… {model_name}: RÂ²={metrics['r2']:.3f}, RMSE={metrics['rmse']:.3f}")
                    
                except Exception as e:
                    logger.error(f"   âŒ Erreur {model_name}: {str(e)}")
                    pollutant_results[model_name] = {'error': str(e)}
            
            results[pollutant] = pollutant_results
        
        # Sauvegarde mÃ©tadonnÃ©es
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'model_type': 'TEMPO_Spatial_Prediction',
            'coverage_area': self.tempo_bounds,
            'pollutants': self.pollutants,
            'models_trained': list(self.model_configs.keys()),
            'feature_columns': feature_columns,
            'results': results
        }
        
        metadata_file = self.models_dir / f"tempo_models_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        return results
    
    def create_geographic_predictor(self, lat: float, lon: float, 
                                   features: Dict[str, float]) -> Dict[str, float]:
        """PrÃ©dit la qualitÃ© de l'air pour n'importe quelles coordonnÃ©es TEMPO"""
        
        if not self.is_in_tempo_coverage(lat, lon):
            raise ValueError(f"CoordonnÃ©es hors zone TEMPO: {lat}, {lon}")
        
        predictions = {}
        
        # CrÃ©ation du vecteur de features pour la prÃ©diction
        feature_vector = self._create_feature_vector(lat, lon, features)
        
        # PrÃ©diction pour chaque polluant
        for pollutant in self.pollutants:
            try:
                # Chargement du meilleur modÃ¨le pour ce polluant
                best_model_file = self._find_best_model(pollutant)
                
                if best_model_file:
                    with open(best_model_file, 'rb') as f:
                        model_data = pickle.load(f)
                    
                    model = model_data['model']
                    scaler = model_data.get('scaler')
                    model_type = model_data.get('model_type')
                    
                    if model_type == 'knn_spatial':
                        # KNN spatial avec coordonnÃ©es
                        coords = np.radians([[lat, lon]])
                        prediction = model.predict(coords)[0]
                    else:
                        # ModÃ¨les avec features complÃ¨tes
                        if scaler:
                            feature_vector_scaled = scaler.transform([feature_vector])
                            prediction = model.predict(feature_vector_scaled)[0]
                        else:
                            prediction = model.predict([feature_vector])[0]
                    
                    predictions[pollutant] = max(0, prediction)  # Valeurs positives seulement
                
            except Exception as e:
                logger.error(f"âŒ Erreur prÃ©diction {pollutant}: {str(e)}")
                predictions[pollutant] = None
        
        return predictions
    
    def _create_feature_vector(self, lat: float, lon: float, features: Dict[str, float]) -> List[float]:
        """CrÃ©e le vecteur de features pour une prÃ©diction"""
        # Features de base gÃ©ographiques
        base_features = {
            'latitude': lat,
            'longitude': lon,
            'lat_rad': np.radians(lat),
            'lon_rad': np.radians(lon),
            'lat_lon_interaction': lat * lon,
            'distance_to_center': np.sqrt((lat - 55.0)**2 + (lon + 100.0)**2),
            'east_coast_proximity': np.exp(-(lon + 75.0)**2 / 100),
            'west_coast_proximity': np.exp(-(lon + 120.0)**2 / 100),
            'great_lakes_proximity': np.exp(-((lat - 45.0)**2 + (lon + 85.0)**2) / 50)
        }
        
        # Fusion avec features fournies
        all_features = {**base_features, **features}
        
        # Ordre des features (doit correspondre Ã  l'entraÃ®nement)
        expected_features = [
            'latitude', 'longitude', 'hour', 'day_of_week', 'month', 'season',
            'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
            'month_sin', 'month_cos', 'temperature', 'humidity', 'pressure',
            'wind_speed', 'wind_direction', 'wind_u', 'wind_v', 'elevation',
            'population_density', 'urban_index', 'land_use_urban',
            'land_use_suburban', 'land_use_rural', 'land_use_forest',
            'pm25_current', 'pm10_current', 'no2_current', 'o3_current',
            'co_current', 'so2_current', 'lat_rad', 'lon_rad',
            'lat_lon_interaction', 'distance_to_center', 'east_coast_proximity',
            'west_coast_proximity', 'great_lakes_proximity'
        ]
        
        feature_vector = []
        for feature_name in expected_features:
            if feature_name in all_features:
                feature_vector.append(all_features[feature_name])
            else:
                feature_vector.append(0.0)  # Valeur par dÃ©faut
        
        return feature_vector
    
    def _find_best_model(self, pollutant: str) -> Optional[str]:
        """Trouve le meilleur modÃ¨le pour un polluant"""
        model_files = list(self.models_dir.glob(f"{pollutant}_*_tempo.pkl"))
        
        if not model_files:
            return None
        
        # Pour simplifier, prendre le dernier crÃ©Ã© (ou implÃ©menter logique de sÃ©lection)
        return str(sorted(model_files, key=lambda x: x.stat().st_mtime, reverse=True)[0])
    
    def is_in_tempo_coverage(self, lat: float, lon: float) -> bool:
        """VÃ©rifie si les coordonnÃ©es sont dans la zone TEMPO"""
        return (self.tempo_bounds['lat_min'] <= lat <= self.tempo_bounds['lat_max'] and
                self.tempo_bounds['lon_min'] <= lon <= self.tempo_bounds['lon_max'])

def main():
    """Pipeline principal d'entraÃ®nement TEMPO"""
    print("ğŸ¤– ENTRAÃNEMENT MODÃˆLES TEMPO SPÃ‰CIALISÃ‰S")
    print("=" * 80)
    print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ›°ï¸ SpÃ©cialisÃ© pour: NASA TEMPO AmÃ©rique du Nord")
    print("=" * 80)
    
    try:
        predictor = TempoMLPredictor()
        
        # 1. Chargement donnÃ©es TEMPO
        print("\nğŸ“‚ Ã‰TAPE 1: Chargement donnÃ©es TEMPO")
        features_df, targets_df = predictor.load_tempo_data()
        
        # 2. EntraÃ®nement modÃ¨les spÃ©cialisÃ©s
        print("\nğŸš€ Ã‰TAPE 2: EntraÃ®nement modÃ¨les spÃ©cialisÃ©s")
        results = predictor.train_spatial_models(features_df, targets_df)
        
        # 3. Test de prÃ©diction gÃ©ographique
        print("\nğŸ§ª Ã‰TAPE 3: Test prÃ©diction gÃ©ographique")
        
        # Test sur quelques villes
        test_locations = [
            (40.7128, -74.0060, "New York"),
            (49.2827, -123.1207, "Vancouver"),
            (41.8781, -87.6298, "Chicago")
        ]
        
        for lat, lon, city in test_locations:
            if predictor.is_in_tempo_coverage(lat, lon):
                # Features de test simulÃ©es
                test_features = {
                    'hour': 14, 'day_of_week': 2, 'month': 10, 'season': 4,
                    'is_weekend': 0, 'temperature': 15.0, 'humidity': 60.0,
                    'pressure': 1013.0, 'wind_speed': 5.0, 'wind_direction': 180.0,
                    'elevation': 100.0, 'population_density': 5000.0,
                    'urban_index': 0.8, 'pm25_current': 12.0
                }
                
                try:
                    predictions = predictor.create_geographic_predictor(lat, lon, test_features)
                    print(f"   ğŸ“ {city} ({lat}, {lon}):")
                    for pollutant, value in predictions.items():
                        if value is not None:
                            print(f"      {pollutant}: {value:.2f}")
                except Exception as e:
                    print(f"   âŒ Erreur {city}: {str(e)}")
        
        # 4. RÃ©sumÃ©
        print("\n" + "=" * 80)
        print("ğŸ‰ MODÃˆLES TEMPO ENTRAÃNÃ‰S!")
        print("=" * 80)
        
        total_models = sum(len([r for r in pollutant_results.values() if 'metrics' in r])
                          for pollutant_results in results.values())
        
        print(f"âœ… ModÃ¨les entraÃ®nÃ©s: {total_models}")
        print(f"âœ… Polluants couverts: {len(results)}")
        print(f"âœ… Zone de couverture: TEMPO AmÃ©rique du Nord")
        
        print(f"\nğŸ¯ Performances par polluant:")
        for pollutant, pollutant_results in results.items():
            best_r2 = -1
            best_model = "Aucun"
            
            for model_name, model_result in pollutant_results.items():
                if 'metrics' in model_result:
                    r2 = model_result['metrics']['r2']
                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = model_name
            
            if best_r2 > -1:
                print(f"   â€¢ {pollutant}: {best_model} (RÂ²={best_r2:.3f})")
        
        print(f"\nğŸ“ ModÃ¨les sauvegardÃ©s: {predictor.models_dir}/")
        print(f"\nğŸ”„ Prochaines Ã©tapes:")
        print(f"   â€¢ IntÃ©grer dans API de prÃ©diction")
        print(f"   â€¢ CrÃ©er endpoints gÃ©ographiques")
        print(f"   â€¢ Optimiser pour temps rÃ©el")
        
    except Exception as e:
        print(f"âŒ Erreur entraÃ®nement: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()